{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ9XfOBWPewb",
        "outputId": "5746870e-a482-48f5-b80e-e039276711d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhqXdemvZsQv"
      },
      "source": [
        "## Extract Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymFUsqx2PO2g",
        "outputId": "41c2abee-0b52-45b5-c7f8-de1aad3186e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/drive/MyDrive/CODE/KHKT/extract_features.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/drive/MyDrive/CODE/KHKT/extract_features.py\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# === C·∫§U H√åNH ===\n",
        "MODEL_PATH = \"/content/drive/MyDrive/CODE/KHKT/checkpoints/segformer_floodnet_epoch15\"\n",
        "TEST_DIR = \"/content/drive/MyDrive/CODE/KHKT/FloodNet/test/img\"\n",
        "SAVE_DIR = \"/content/drive/MyDrive/CODE/KHKT/FloodNet/test_results\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Load processor\n",
        "try:\n",
        "    processor = SegformerImageProcessor.from_pretrained(\n",
        "        MODEL_PATH,\n",
        "        local_files_only=True\n",
        "    )\n",
        "    print(\"Loaded processor from checkpoint.\")\n",
        "except:\n",
        "    processor = SegformerImageProcessor.from_pretrained(\n",
        "        \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "        do_reduce_labels=False,\n",
        "        size={\"height\": 512, \"width\": 512}\n",
        "    )\n",
        "    print(\"Loaded default base processor.\")\n",
        "\n",
        "# Label mapping (FloodNet 0‚Äì9)\n",
        "id2label = {\n",
        "    0: \"background\",\n",
        "    1: \"building_flooded\",\n",
        "    2: \"building_non_flooded\",\n",
        "    3: \"road_flooded\",\n",
        "    4: \"road_non_flooded\",\n",
        "    5: \"water\",\n",
        "    6: \"tree\",\n",
        "    7: \"vehicle\",\n",
        "    8: \"pool\",\n",
        "    9: \"grass\",\n",
        "}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "# Load model\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    num_labels=10,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True,\n",
        "    local_files_only=True\n",
        ")\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Color map\n",
        "id2color = {\n",
        "    0: (0, 0, 0),\n",
        "    1: (0, 0, 128),\n",
        "    2: (0, 0, 255),\n",
        "    3: (255, 255, 0),\n",
        "    4: (128, 128, 128),\n",
        "    5: (0, 255, 255),\n",
        "    6: (0, 128, 0),\n",
        "    7: (255, 0, 0),\n",
        "    8: (128, 0, 0),\n",
        "    9: (0, 255, 0),\n",
        "}\n",
        "\n",
        "# === H√ÄM INFERENCE ===\n",
        "def predict_flood_level(image_path):\n",
        "    # 1. ƒê·ªçc ·∫£nh\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # 2. Preprocess\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    pixel_values = inputs.pixel_values.to(device)\n",
        "\n",
        "    # 3. Inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(pixel_values)\n",
        "        logits = outputs.logits  # (1, 10, h, w)\n",
        "\n",
        "    # 4. Upsample v·ªÅ k√≠ch th∆∞·ªõc ·∫£nh g·ªëc\n",
        "    upsampled_logits = F.interpolate(\n",
        "        logits,\n",
        "        size=image.size[::-1],  # (H, W)\n",
        "        mode=\"bilinear\",\n",
        "        align_corners=False\n",
        "    )\n",
        "    seg = upsampled_logits.argmax(dim=1)[0].cpu().numpy()  # (H, W)\n",
        "\n",
        "    # 5. T√≠nh t·ªâ l·ªá n∆∞·ªõc\n",
        "    flood_classes = [1, 3, 5]\n",
        "    water_ratio = np.isin(seg, flood_classes).sum() / seg.size\n",
        "\n",
        "\n",
        "    # 6. Ph√¢n m·ª©c ng·∫≠p theo t·ªâ l·ªá water\n",
        "    if water_ratio < 0.05:\n",
        "        level, level_text = 0, \"Kh√¥ng ng·∫≠p\"\n",
        "    elif water_ratio < 0.1:\n",
        "        level, level_text = 1, \"Ng·∫≠p nh·∫π\"\n",
        "    elif water_ratio < 0.3:\n",
        "        level, level_text = 2, \"Ng·∫≠p v·ª´a\"\n",
        "    else:\n",
        "        level, level_text = 3, \"Ng·∫≠p n·∫∑ng\"\n",
        "\n",
        "    # 7. T·∫°o mask m√†u ƒë·∫πp\n",
        "    h, w = seg.shape\n",
        "    mask_colored = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for label, color in id2color.items():\n",
        "        mask_colored[seg == label] = color\n",
        "\n",
        "    return {\n",
        "        \"original\": image,\n",
        "        \"segmentation\": mask_colored,\n",
        "        \"seg_map\": seg,\n",
        "        \"water_ratio\": water_ratio,\n",
        "        \"flood_level\": level,\n",
        "        \"flood_text\": level_text\n",
        "    }\n",
        "\n",
        "# C√°c class ng·∫≠p\n",
        "FLOOD_CLASSES = [1, 3, 5]  # building_flooded, road_flooded, water\n",
        "\n",
        "def extract_features(image_path):\n",
        "    \"\"\"\n",
        "    Tr√≠ch xu·∫•t feature ƒë·ªÉ l√†m input cho m√¥ h√¨nh d·ª± b√°o ng·∫≠p t∆∞∆°ng lai.\n",
        "    \"\"\"\n",
        "    result = predict_flood_level(image_path)\n",
        "    seg_map = result[\"seg_map\"]\n",
        "\n",
        "    # 1) C√°c t·ª∑ l·ªá pixel\n",
        "    total = seg_map.size\n",
        "    ratios = {\n",
        "        \"water_ratio\": np.mean(seg_map == 5),\n",
        "        \"building_flooded_ratio\": np.mean(seg_map == 1),\n",
        "        \"road_flooded_ratio\": np.mean(seg_map == 3),\n",
        "        \"overall_flood_ratio\": np.mean(np.isin(seg_map, FLOOD_CLASSES)),\n",
        "        \"tree_ratio\": np.mean(seg_map == 6),\n",
        "        \"vehicle_ratio\": np.mean(seg_map == 7),\n",
        "        \"pool_ratio\": np.mean(seg_map == 8),\n",
        "        \"grass_ratio\": np.mean(seg_map == 9),\n",
        "    }\n",
        "\n",
        "    # 2) Tr√≠ch xu·∫•t feature t·ª´ encoder c·ªßa SegFormer\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(images=img, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "        last_feat = outputs.hidden_states[-1]     # (1, C, H', W')\n",
        "        pooled_feat = last_feat.mean(dim=[2, 3])  # => (1, C)\n",
        "        pooled_feat = pooled_feat.cpu().numpy().flatten()\n",
        "\n",
        "    # Tr·∫£ v·ªÅ dictionary + vector feature\n",
        "    return ratios, pooled_feat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyEwW2vxZ0wC"
      },
      "source": [
        "## Build future dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdxvTPlGw3Ta",
        "outputId": "f330fffe-f345-436d-a2bf-82d2cc713b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/CODE/KHKT/build_synthetic_physics_dataset.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/drive/MyDrive/CODE/KHKT/build_synthetic_physics_dataset.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"/content/drive/MyDrive/CODE/KHKT\")\n",
        "from extract_features import extract_features  # d√πng h√†m b·∫°n ƒë√£ c√≥\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/CODE/KHKT/FloodNet/validation/img\"\n",
        "\n",
        "# === H√ÄM T·∫†O PROFILE M∆ØA B·∫¨C 1 (LINEAR) ===\n",
        "def generate_rain_profile(r0):\n",
        "    \"\"\"\n",
        "    Sinh l∆∞·ª£ng m∆∞a 1h, 3h, 6h, 12h theo m√¥ h√¨nh tuy·∫øn t√≠nh:\n",
        "        rain(t) = r0 + slope * t + noise\n",
        "    ƒêi·ªÅu ki·ªán:\n",
        "        - slope random nh·ªè ƒë·ªÉ m∆∞a thay ƒë·ªïi kh√¥ng ƒë·ªôt ng·ªôt\n",
        "        - noise gi·ªõi h·∫°n ¬±1 mm/h\n",
        "        - kh√¥ng v∆∞·ª£t qu√° r0 ¬± 10\n",
        "        - kh√¥ng v∆∞·ª£t qu√° [0, 70]\n",
        "    \"\"\"\n",
        "\n",
        "    # -------- 1. Random slope --------\n",
        "    # Cho r0 nh·ªè ‚Üí slope nh·ªè\n",
        "    # Cho r0 l·ªõn ‚Üí slope c√≥ th·ªÉ √¢m (m∆∞a gi·∫£m)\n",
        "    slope_range = 0.10 + (20 - r0) / 200   # gi·∫£m khi r0 l·ªõn\n",
        "    slope = np.random.uniform(-slope_range, slope_range)\n",
        "\n",
        "    # -------- 2. Th·ªùi gian (gi·ªù) --------\n",
        "    times = np.array([1, 3, 6, 12], dtype=float)\n",
        "\n",
        "    # -------- 3. Noise r·∫•t nh·ªè ƒë·ªÉ profile tr√¥ng th·∫≠t --------\n",
        "    noise = np.random.uniform(-1.0, 1.0, size=4)\n",
        "\n",
        "    # -------- 4. T√≠nh l∆∞·ª£ng m∆∞a --------\n",
        "    r = r0 + slope * times + noise\n",
        "\n",
        "    # -------- 5. Gi·ªõi h·∫°n v·∫≠t l√Ω --------\n",
        "    r_min = max(r0 - 10.0, 0.0)\n",
        "    r_max = min(r0 + 10.0, 70.0)\n",
        "    r = np.clip(r, r_min, r_max)\n",
        "\n",
        "    return float(r[0]), float(r[1]), float(r[2]), float(r[3])\n",
        "\n",
        "# === H√ÄM V·∫¨T L√ù T√çNH TI·∫æN TRI·ªÇN NG·∫¨P ===\n",
        "\n",
        "def compute_physics_based_progression(ratios, flood_prev, rain_prev, rain_next, dt_hours):\n",
        "    \"\"\"\n",
        "    M√¥ ph·ªèng flood progression t·ª´ t1 ‚Üí t2 theo m∆∞a d·ª± b√°o.\n",
        "    dt_hours: s·ªë gi·ªù tr√¥i qua gi·ªØa 2 m·ªëc.\n",
        "    \"\"\"\n",
        "    # 1. Rain average\n",
        "    rain_avg = (rain_prev + rain_next) / 2.0\n",
        "\n",
        "    # 2. Rain effect (mm/h -> flood delta)\n",
        "    if rain_avg < 2:\n",
        "        rain_effect_per_hour = np.interp(rain_avg, [0, 2], [-0.03, -0.01])\n",
        "    elif rain_avg < 5:\n",
        "        rain_effect_per_hour = np.interp(rain_avg, [2, 5], [0.0, 0.02])\n",
        "    elif rain_avg < 12:\n",
        "        rain_effect_per_hour = np.interp(rain_avg, [5, 12], [0.02, 0.05])\n",
        "    elif rain_avg < 20:\n",
        "        rain_effect_per_hour = np.interp(rain_avg, [12, 20], [0.05, 0.08])\n",
        "    else:\n",
        "        rain_effect_per_hour = np.interp(min(rain_avg, 100), [20, 100], [0.08, 0.10])\n",
        "\n",
        "    # T·ªïng hi·ªáu ·ª©ng m∆∞a theo th·ªùi gian dt\n",
        "    rain_total = rain_effect_per_hour * dt_hours\n",
        "\n",
        "    drainage_total = -0.05 * dt_hours\n",
        "\n",
        "    # 3. Flood next = flood_prev + effects\n",
        "    flood_next = flood_prev + rain_total + drainage_total\n",
        "\n",
        "    # √©p trong [0,1]\n",
        "    flood_next = float(np.clip(flood_next, 0.0, 1.0))\n",
        "\n",
        "    return flood_next\n",
        "\n",
        "# === BUILD DATASET ===\n",
        "data = []\n",
        "\n",
        "print(\"üîß ƒêang t·∫°o synthetic_physics_multistep_dataset.pkl ...\")\n",
        "\n",
        "for img_name in tqdm(os.listdir(IMAGE_DIR)):\n",
        "    if not img_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        continue\n",
        "\n",
        "    img_path = os.path.join(IMAGE_DIR, img_name)\n",
        "\n",
        "    ratios, enc = extract_features(img_path)\n",
        "    cur_flood = ratios[\"overall_flood_ratio\"]\n",
        "\n",
        "    # random m∆∞a hi·ªán t·∫°i (0..50 mm/h)\n",
        "    rain_now = float(np.random.uniform(0, 50))\n",
        "\n",
        "    # t·∫°o profile m∆∞a theo b·∫≠c 2\n",
        "    rain_1h, rain_3h, rain_6h, rain_12h = generate_rain_profile(rain_now)\n",
        "\n",
        "    # ti·∫øn tri·ªÉn ng·∫≠p theo t·ª´ng m·ªëc (d√πng d·∫°ng \"chu·ªói th·ªùi gian\")\n",
        "    flood_now = cur_flood\n",
        "\n",
        "    # 0h ‚Üí 1h (dt = 1h)\n",
        "    flood_1h = compute_physics_based_progression(\n",
        "        ratios, flood_now, rain_now, rain_1h, dt_hours=1\n",
        "    )\n",
        "\n",
        "    # 1h ‚Üí 3h (dt = 2h)\n",
        "    flood_3h = compute_physics_based_progression(\n",
        "        ratios, flood_1h, rain_1h, rain_3h, dt_hours=2\n",
        "    )\n",
        "\n",
        "    # 3h ‚Üí 6h (dt = 3h)\n",
        "    flood_6h = compute_physics_based_progression(\n",
        "        ratios, flood_3h, rain_3h, rain_6h, dt_hours=3\n",
        "    )\n",
        "\n",
        "    # 6h ‚Üí 12h (dt = 6h)\n",
        "    flood_12h = compute_physics_based_progression(\n",
        "        ratios, flood_6h, rain_6h, rain_12h, dt_hours=6\n",
        "    )\n",
        "\n",
        "    row = {\n",
        "        \"filename\": img_name,\n",
        "        \"rain_now\": rain_now,\n",
        "        \"rain_1h\": rain_1h,\n",
        "        \"rain_3h\": rain_3h,\n",
        "        \"rain_6h\": rain_6h,\n",
        "        \"rain_12h\": rain_12h,\n",
        "        \"flood_now\": flood_now,\n",
        "        \"flood_1h\": flood_1h,\n",
        "        \"flood_3h\": flood_3h,\n",
        "        \"flood_6h\": flood_6h,\n",
        "        \"flood_12h\": flood_12h,\n",
        "        **ratios,\n",
        "        \"encoder_feature\": enc,\n",
        "    }\n",
        "\n",
        "    data.append(row)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "out_path = \"/content/drive/MyDrive/CODE/KHKT/synthetic_physics_multistep_dataset.pkl\"\n",
        "df.to_pickle(out_path)\n",
        "\n",
        "print(\"üéâ HO√ÄN T·∫§T:\", out_path)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkv4j-UpyuXu"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/CODE/KHKT\n",
        "!python build_synthetic_physics_dataset.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS5hInvdau22"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68Wlk80JY5iq",
        "outputId": "7d6a1716-ebaf-4660-fe1f-ddeb9922c535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE (multi-output): 0.006246772216803885\n",
            "ƒê√£ l∆∞u model: /content/drive/MyDrive/CODE/KHKT/future_flood_multistep.pkl\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/CODE/KHKT/synthetic_physics_multistep_dataset.pkl\"\n",
        "df = pd.read_pickle(DATA_PATH)\n",
        "\n",
        "# t√°ch encoder_feature\n",
        "enc_mat = np.stack(df[\"encoder_feature\"].values)\n",
        "\n",
        "semantic_cols = [\n",
        "    \"water_ratio\",\n",
        "    \"building_flooded_ratio\",\n",
        "    \"road_flooded_ratio\",\n",
        "    \"overall_flood_ratio\",\n",
        "    \"tree_ratio\",\n",
        "    \"vehicle_ratio\",\n",
        "    \"pool_ratio\",\n",
        "    \"grass_ratio\",\n",
        "    \"rain_now\",\n",
        "    \"rain_1h\",\n",
        "    \"rain_3h\",\n",
        "    \"rain_6h\",\n",
        "    \"rain_12h\",\n",
        "]\n",
        "\n",
        "X_sem = df[semantic_cols].values\n",
        "X = np.concatenate([X_sem, enc_mat], axis=1)\n",
        "\n",
        "y = df[[\"flood_1h\", \"flood_3h\", \"flood_6h\", \"flood_12h\"]].values\n",
        "\n",
        "model = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "model.fit(X, y)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "print(\"MAE (multi-output):\", mae)\n",
        "\n",
        "out_model = \"/content/drive/MyDrive/CODE/KHKT/future_flood_multistep.pkl\"\n",
        "joblib.dump(model, out_model)\n",
        "print(\"ƒê√£ l∆∞u model:\", out_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iNQWw80bar3"
      },
      "source": [
        "## T·∫°o dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C9ZDRqFbb1zt",
        "outputId": "6a2f6ac5-c8c3-41ec-ab86-b441388d5a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.51.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.51.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.51.0\n"
          ]
        }
      ],
      "source": [
        "pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj6bZRKvXEP6",
        "outputId": "80fa2c89-15c9-43b2-9ad4-a712a5be598d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/CODE/KHKT/app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/drive/MyDrive/CODE/KHKT/app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import joblib\n",
        "from PIL import Image\n",
        "from tempfile import NamedTemporaryFile\n",
        "import requests\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "from io import BytesIO\n",
        "from difflib import get_close_matches\n",
        "import unicodedata\n",
        "\n",
        "# ADD MODULE PATH (ƒë·ªÉ import extract_features.py)\n",
        "sys.path.append(\"/content/drive/MyDrive/CODE/KHKT\")\n",
        "\n",
        "# Import segmentation model & feature extractor\n",
        "from extract_features import extract_features, predict_flood_level\n",
        "\n",
        "# Load future flood predictor model\n",
        "predictor_model = joblib.load(\"/content/drive/MyDrive/CODE/KHKT/future_flood_multistep.pkl\")\n",
        "\n",
        "# WEATHER API KEY\n",
        "API_KEY = \"b3e86069b418794e0effd1c9d29c2466\"\n",
        "\n",
        "# 1. DANH S√ÅCH T·ªàNH TH√ÄNH VI·ªÜT NAM + G·ª¢I √ù\n",
        "VIETNAM_PROVINCES = [\n",
        "    \"An Giang\", \"Ba Ria - Vung Tau\", \"Bac Giang\", \"Bac Kan\", \"Bac Lieu\", \"Bac Ninh\",\n",
        "    \"Ben Tre\", \"Binh Dinh\", \"Binh Duong\", \"Binh Phuoc\", \"Binh Thuan\",\n",
        "    \"Ca Mau\", \"Can Tho\", \"Cao Bang\",\n",
        "    \"Da Nang\", \"Dak Lak\", \"Dak Nong\", \"Dien Bien\", \"Dong Nai\", \"Dong Thap\",\n",
        "    \"Gia Lai\", \"Ha Giang\", \"Ha Nam\", \"Ha Noi\", \"Ha Tinh\",\n",
        "    \"Hai Duong\", \"Hai Phong\", \"Hau Giang\", \"Hoa Binh\",\n",
        "    \"Ho Chi Minh City\", \"HCM\",\n",
        "    \"Hung Yen\", \"Khanh Hoa\", \"Kien Giang\", \"Kon Tum\",\n",
        "    \"Lai Chau\", \"Lam Dong\", \"Lang Son\", \"Lao Cai\", \"Long An\",\n",
        "    \"Nam Dinh\", \"Nghe An\", \"Ninh Binh\", \"Ninh Thuan\",\n",
        "    \"Phu Tho\", \"Phu Yen\", \"Quang Binh\", \"Quang Nam\", \"Quang Ngai\",\n",
        "    \"Quang Ninh\", \"Quang Tri\",\n",
        "    \"Soc Trang\", \"Son La\", \"Tay Ninh\", \"Thai Binh\", \"Thai Nguyen\",\n",
        "    \"Thanh Hoa\", \"Thua Thien Hue\", \"Tien Giang\", \"Tra Vinh\",\n",
        "    \"Tuyen Quang\", \"Vinh Long\", \"Vinh Phuc\",\n",
        "    \"Yen Bai\"\n",
        "]\n",
        "\n",
        "def normalize_text(s):\n",
        "    s = s.lower().strip()\n",
        "    s = unicodedata.normalize('NFD', s)\n",
        "    return ''.join(ch for ch in s if unicodedata.category(ch) != 'Mn')\n",
        "\n",
        "def suggest_locations(text):\n",
        "    text_norm = normalize_text(text)\n",
        "    province_norm_map = {normalize_text(p): p for p in VIETNAM_PROVINCES}\n",
        "    matches = get_close_matches(text_norm, province_norm_map.keys(), n=5, cutoff=0.4)\n",
        "    return [province_norm_map[m] for m in matches]\n",
        "\n",
        "\n",
        "# 2. GEOLOCATION API (OpenWeather)\n",
        "def get_coordinates(location_name):\n",
        "    \"\"\"Chuy·ªÉn t√™n t·ªânh -> lat, lon c√≥ fuzzy fix.\"\"\"\n",
        "    name = location_name.strip()\n",
        "    url = f\"http://api.openweathermap.org/geo/1.0/direct?q={name}&limit=5&appid={API_KEY}\"\n",
        "    r = requests.get(url).json()\n",
        "\n",
        "    if not r:\n",
        "        # N·∫øu l·ªói -> th·ª≠ fuzzy suggestion\n",
        "        suggestions = suggest_locations(name)\n",
        "        if not suggestions:\n",
        "            return None, None\n",
        "\n",
        "        fixed = suggestions[0]\n",
        "        url2 = f\"http://api.openweathermap.org/geo/1.0/direct?q={fixed}&limit=5&appid={API_KEY}\"\n",
        "        r2 = requests.get(url2).json()\n",
        "        if not r2:\n",
        "            return None, None\n",
        "        return r2[0][\"lat\"], r2[0][\"lon\"]\n",
        "\n",
        "    return r[0][\"lat\"], r[0][\"lon\"]\n",
        "\n",
        "\n",
        "# 3. RAINFALL API (PRECIPITATION TILE)\n",
        "import requests\n",
        "API_KEY_WAPI = \"d243e111150c4739a80135517252711\"\n",
        "\n",
        "def get_rain_forecast(lat, lon, api_key):\n",
        "    \"\"\"\n",
        "    Tr·∫£ v·ªÅ 5 gi√° tr·ªã:\n",
        "    - rain_now (mm/h)\n",
        "    - rain_1h\n",
        "    - rain_3h\n",
        "    - rain_6h\n",
        "    - rain_12h\n",
        "    T·∫•t c·∫£ ƒë·ªÅu t·ª´ WeatherAPI forecast, 100% theo th·ªùi gian th·∫≠t.\n",
        "    \"\"\"\n",
        "\n",
        "    url = (\n",
        "        f\"https://api.weatherapi.com/v1/forecast.json\"\n",
        "        f\"?key={API_KEY_WAPI}&q={lat},{lon}&hours=12\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        js = requests.get(url).json()\n",
        "    except Exception as e:\n",
        "        print(\"Error calling WeatherAPI:\", e)\n",
        "        return (0.0, 0.0, 0.0, 0.0, 0.0)\n",
        "\n",
        "    # 1) M∆∞a hi·ªán t·∫°i\n",
        "    rain_now = js.get(\"current\", {}).get(\"precip_mm\", 0.0)\n",
        "\n",
        "    # 2) List gi·ªù ti·∫øp theo\n",
        "    hours = js.get(\"forecast\", {}).get(\"forecastday\", [{}])[0].get(\"hour\", [])\n",
        "\n",
        "    # ƒê·∫£m b·∫£o ƒë·ªß 12 gi·ªù\n",
        "    def safe_precip(index):\n",
        "        if index < len(hours):\n",
        "            return hours[index].get(\"precip_mm\", 0.0)\n",
        "        return 0.0\n",
        "\n",
        "    rain_1h  = safe_precip(1)   # gi·ªù th·ª© 1\n",
        "    rain_3h  = safe_precip(3)\n",
        "    rain_6h  = safe_precip(6)\n",
        "    rain_12h = safe_precip(12)\n",
        "\n",
        "    return (\n",
        "        float(rain_now),\n",
        "        float(rain_1h),\n",
        "        float(rain_3h),\n",
        "        float(rain_6h),\n",
        "        float(rain_12h)\n",
        "    )\n",
        "\n",
        "# STREAMLIT UI\n",
        "st.set_page_config(page_title=\"AI C·∫£nh B√°o Ng·∫≠p L·ª•t\", layout=\"wide\")\n",
        "st.title(\"üåß AI C·∫£nh B√°o Ng·∫≠p L·ª•t ‚Äî Realtime Weather + Segmentation\")\n",
        "\n",
        "st.write(\"D·ª± b√°o ng·∫≠p t∆∞∆°ng lai d·ª±a tr√™n ·∫£nh, segmentation v√† d·ªØ li·ªáu m∆∞a th·ªùi gian th·ª±c.\")\n",
        "\n",
        "\n",
        "# LOCATION INPUT\n",
        "st.subheader(\"üìç Ch·ªçn v·ªã tr√≠ l·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt\")\n",
        "\n",
        "mode = st.radio(\"Ch·ªçn c√°ch nh·∫≠p:\", [\"Nh·∫≠p t√™n t·ªânh (g·ª£i √Ω t·ª± ƒë·ªông)\", \"Nh·∫≠p t·ªça ƒë·ªô\"])\n",
        "\n",
        "lat, lon = None, None\n",
        "\n",
        "if mode == \"Nh·∫≠p t√™n t·ªânh (g·ª£i √Ω t·ª± ƒë·ªông)\":\n",
        "    user_input = st.text_input(\"Nh·∫≠p t√™n t·ªânh/th√†nh ph·ªë (VD: Quy Nhon, Ho Chi Minh City):\")\n",
        "\n",
        "    if user_input:\n",
        "        suggestions = suggest_locations(user_input)\n",
        "        if len(suggestions) == 0:\n",
        "            st.error(\"‚ùå Kh√¥ng t√¨m th·∫•y ƒë·ªãa ƒëi·ªÉm ph√π h·ª£p.\")\n",
        "        else:\n",
        "            chosen = st.selectbox(\"G·ª£i √Ω:\", suggestions)\n",
        "            lat, lon = get_coordinates(chosen)\n",
        "            if lat:\n",
        "                st.success(f\"üìå T·ªça ƒë·ªô: {lat}, {lon}\")\n",
        "\n",
        "else:\n",
        "    lat = st.number_input(\"Latitude:\", format=\"%.6f\")\n",
        "    lon = st.number_input(\"Longitude:\", format=\"%.6f\")\n",
        "\n",
        "if lat is None or lon is None:\n",
        "    st.stop()\n",
        "\n",
        "\n",
        "# WEATHER: RADAR RAIN\n",
        "st.subheader(\"üåß L∆∞·ª£ng m∆∞a hi·ªán t·∫°i:\")\n",
        "rain_now, rain_1h, rain_3h, rain_6h, rain_12h = get_rain_forecast(lat, lon, API_KEY_WAPI)\n",
        "st.metric(\"Rain Radar (mm/h)\", f\"{rain_now:.2f}\")\n",
        "st.subheader(\"üåß L∆∞·ª£ng m∆∞a d·ª± b√°o:\")\n",
        "st.write(f\"Trong 1 gi·ªù: {rain_1h:.2f} mm/h\")\n",
        "st.write(f\"Trong 3 gi·ªù: {rain_3h:.2f} mm/h\")\n",
        "st.write(f\"Trong 6 gi·ªù: {rain_6h:.2f} mm/h\")\n",
        "st.write(f\"Trong 12 gi·ªù: {rain_12h:.2f} mm/h\")\n",
        "\n",
        "# IMAGE UPLOAD\n",
        "uploaded = st.file_uploader(\"üì∏ T·∫£i ·∫£nh ƒë·ªÉ ph√¢n t√≠ch ng·∫≠p\", type=[\"jpg\",\"jpeg\",\"png\"])\n",
        "\n",
        "if uploaded:\n",
        "    img = Image.open(uploaded).convert(\"RGB\")\n",
        "\n",
        "    with NamedTemporaryFile(delete=False, suffix=\".png\") as tmp:\n",
        "        img.save(tmp.name)\n",
        "\n",
        "        # segmentation\n",
        "        seg_result = predict_flood_level(tmp.name)\n",
        "\n",
        "        # feature extraction\n",
        "        ratios, enc = extract_features(tmp.name)\n",
        "\n",
        "    # UI hi·ªÉn th·ªã segmentation\n",
        "    seg_img = seg_result[\"segmentation\"]\n",
        "    water_ratio = seg_result[\"water_ratio\"]\n",
        "    level_now = seg_result[\"flood_text\"]\n",
        "\n",
        "    colA, colB = st.columns(2)\n",
        "    with colA:\n",
        "        st.image(img, caption=\"·∫¢nh g·ªëc\", use_container_width=True)\n",
        "    with colB:\n",
        "        st.image(seg_img, caption=f\"Segmentation ‚Äî Ng·∫≠p: {water_ratio:.1%}\", use_container_width=True)\n",
        "\n",
        "    st.write(f\"### üåä M·ª©c ng·∫≠p hi·ªán t·∫°i: **{level_now}**\")\n",
        "\n",
        "    semantic = np.array([\n",
        "        ratios[\"water_ratio\"],\n",
        "        ratios[\"building_flooded_ratio\"],\n",
        "        ratios[\"road_flooded_ratio\"],\n",
        "        ratios[\"overall_flood_ratio\"],\n",
        "        ratios[\"tree_ratio\"],\n",
        "        ratios[\"vehicle_ratio\"],\n",
        "        ratios[\"pool_ratio\"],\n",
        "        ratios[\"grass_ratio\"],\n",
        "        rain_now,\n",
        "        rain_1h,\n",
        "        rain_3h,\n",
        "        rain_6h,\n",
        "        rain_12h,\n",
        "    ])\n",
        "\n",
        "    X_input = np.concatenate([semantic, enc])[None, :]\n",
        "    future_vec = predictor_model.predict(X_input)[0]  # [flood_1h, flood_3h, flood_6h, flood_12h]\n",
        "\n",
        "    st.subheader(\"‚è± Ch·ªçn m·ªëc th·ªùi gian d·ª± b√°o\")\n",
        "    option = st.radio(\n",
        "        \"M·ªëc d·ª± b√°o:\",\n",
        "        [\"1 gi·ªù\", \"3 gi·ªù\", \"6 gi·ªù\", \"12 gi·ªù\"],\n",
        "        horizontal=True\n",
        "    )\n",
        "\n",
        "    index_map = {\"1 gi·ªù\": 0, \"3 gi·ªù\": 1, \"6 gi·ªù\": 2, \"12 gi·ªù\": 3}\n",
        "    future_ratio = float(future_vec[index_map[option]])\n",
        "\n",
        "    if future_ratio < 0.05:\n",
        "        future_text = \"Kh√¥ng ng·∫≠p\"\n",
        "    elif future_ratio < 0.15:\n",
        "        future_text = \"Ng·∫≠p nh·∫π\"\n",
        "    elif future_ratio < 0.35:\n",
        "        future_text = \"Ng·∫≠p v·ª´a\"\n",
        "    else:\n",
        "        future_text = \"Ng·∫≠p n·∫∑ng\"\n",
        "\n",
        "    st.metric(f\"M·ª©c ng·∫≠p sau {option}: \", future_text, f\"{future_ratio*100:.1f}%\")\n",
        "\n",
        "    if future_ratio >= 0.35:\n",
        "        st.error(\"‚ö† C·∫£nh b√°o ng·∫≠p n·∫∑ng!\")\n",
        "    elif future_ratio >= 0.15:\n",
        "        st.warning(\"‚ö† Nguy c∆° ng·∫≠p ‚Äî n√™n theo d√µi.\")\n",
        "    else:\n",
        "        st.success(\"‚úî An to√†n ‚Äî Nguy c∆° th·∫•p.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFCES8oabd5M",
        "outputId": "a9f85f62-0d02-4198-937b-e13979900ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-28 06:42:31--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.11.1/cloudflared-linux-amd64.deb [following]\n",
            "--2025-11-28 06:42:31--  https://github.com/cloudflare/cloudflared/releases/download/2025.11.1/cloudflared-linux-amd64.deb\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/106867604/8a32f7c6-649c-4f0d-806d-e14c19d0786d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-28T07%3A24%3A14Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64.deb&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-28T06%3A23%3A25Z&ske=2025-11-28T07%3A24%3A14Z&sks=b&skv=2018-11-09&sig=5wbt7SGpwrnEcaodUVJMmMIAHWfj7BBfFVubz9Rphec%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDMxMzk1MSwibmJmIjoxNzY0MzEyMTUxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.HJVfxmvSwHAD6j3wnlRvDWMINHVoxzRcTAw6K0MalhU&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64.deb&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-11-28 06:42:31--  https://release-assets.githubusercontent.com/github-production-release-asset/106867604/8a32f7c6-649c-4f0d-806d-e14c19d0786d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-28T07%3A24%3A14Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64.deb&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-28T06%3A23%3A25Z&ske=2025-11-28T07%3A24%3A14Z&sks=b&skv=2018-11-09&sig=5wbt7SGpwrnEcaodUVJMmMIAHWfj7BBfFVubz9Rphec%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDMxMzk1MSwibmJmIjoxNzY0MzEyMTUxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.HJVfxmvSwHAD6j3wnlRvDWMINHVoxzRcTAw6K0MalhU&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64.deb&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20192500 (19M) [application/octet-stream]\n",
            "Saving to: ‚Äòcloudflared-linux-amd64.deb‚Äô\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  19.26M  24.2MB/s    in 0.8s    \n",
            "\n",
            "2025-11-28 06:42:32 (24.2 MB/s) - ‚Äòcloudflared-linux-amd64.deb‚Äô saved [20192500/20192500]\n",
            "\n",
            "Selecting previously unselected package cloudflared.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
            "Unpacking cloudflared (2025.11.1) ...\n",
            "Setting up cloudflared (2025.11.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "cloudflared version 2025.11.1 (built 2025-11-07-16:59 UTC)\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "!cloudflared --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xWqOwhMblk6"
      },
      "source": [
        "## Run Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6UhfGSubkzM",
        "outputId": "ce6a56b6-3a01-4b59-b236-4ba13445db20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CODE/KHKT\n",
            "\u001b[90m2025-11-28T06:42:37Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-11-28T06:42:37Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m |  https://office-employers-princeton-managed.trycloudflare.com                              |\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 83ea55259e419549817460d0c097f23ad1327364d0a63fab2c5463b9283251cb)\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update if installed by a package manager.\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 863dc1fc-de98-4258-9c32-fb045710ec5d\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-11-28T06:42:40Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "2025/11/28 06:42:40 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-11-28T06:42:41Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m2c357ba2-7b3f-46eb-ab0e-990ee7ed13c9 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7 \u001b[36mlocation=\u001b[0miad15 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.145.147.141:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-11-28 06:44:21.337974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764312261.368805    6407 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764312261.377685    6407 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764312261.400295    6407 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764312261.400349    6407 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764312261.400354    6407 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764312261.400361    6407 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 06:44:21.406936: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loaded processor from checkpoint.\n",
            "2025-11-28 07:08:07.251 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:08:07.261 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:14:28.175 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:14:28.184 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:14:37.830 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:14:37.844 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:14:42.896 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:14:42.905 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:15:40.234 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:15:40.588 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:16:08.023 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:16:08.388 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:16:30.458 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:16:30.806 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:18:30.821 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:18:31.168 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:18:53.890 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:18:54.241 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:19:45.109 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:19:45.460 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:20:28.819 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:20:29.173 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:21:04.631 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:21:04.997 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:22:41.515 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:22:41.863 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:22:50.729 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:22:50.739 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:23:41.250 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:23:41.260 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:24:05.691 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:24:05.701 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:24:31.257 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-28 07:24:31.266 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[90m2025-11-28T07:41:17Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2025-11-28T07:41:17Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "\u001b[90m2025-11-28T07:41:17Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "\u001b[90m2025-11-28T07:41:17Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "\u001b[90m2025-11-28T07:41:17Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "\u001b[90m2025-11-28T07:41:17Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2025-11-28T07:41:17Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2025-11-28T07:41:17Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2025-11-28T07:41:17Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CODE/KHKT\n",
        "!streamlit run app.py & cloudflared tunnel --url http://localhost:8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dhqXdemvZsQv",
        "lyEwW2vxZ0wC",
        "mS5hInvdau22"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}