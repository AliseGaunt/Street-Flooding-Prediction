{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "Y9xWsO8w-Q-W",
        "s1ScLJYy3GpN",
        "9_G47U6X3Pc5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Set up"
      ],
      "metadata": {
        "id": "Y9xWsO8w-Q-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7oDPJ5lCnoo",
        "outputId": "78ed5949-71c4-450a-aee6-f6d7966e2083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwutgd7w0iQV"
      },
      "outputs": [],
      "source": [
        "# FLOODNET SEGMENTATION\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import SegformerForSemanticSegmentation\n",
        "from transformers import SegformerImageProcessor  # ĐÃ THAY SegformerFeatureExtractor → ImageProcessor\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torchmetrics import JaccardIndex # pip install torchmetrics nếu chưa có\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Tạo folder checkpoints trong Drive (nếu chưa có)\n",
        "!mkdir -p \"/content/drive/MyDrive/CODE/KHKT/checkpoints\"\n",
        "\n",
        "# Tạo symbolic link từ /content/checkpoints → Drive\n",
        "!ln -sfn \"/content/drive/MyDrive/CODE/KHKT/checkpoints\" \"/content/checkpoints\"\n",
        "\n",
        "# Kiểm tra (phải thấy đường dẫn trỏ về Drive)\n",
        "!ls -la /content/checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRDY_0scHgcV",
        "outputId": "340d34ff-5cf9-4af0-8750-c04f52ff4fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "lrwxrwxrwx 1 root root 44 Nov 27 01:20 /content/checkpoints -> /content/drive/MyDrive/CODE/KHKT/checkpoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cấu hình Class"
      ],
      "metadata": {
        "id": "s1ScLJYy3GpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    0: \"background\",\n",
        "    1: \"building_flooded\",\n",
        "    2: \"building_non_flooded\",\n",
        "    3: \"road_flooded\",\n",
        "    4: \"road_non_flooded\",\n",
        "    5: \"water\",\n",
        "    6: \"tree\",\n",
        "    7: \"vehicle\",\n",
        "    8: \"pool\",\n",
        "    9: \"grass\",\n",
        "}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "num_labels = len(id2label)\n",
        "\n",
        "def rgb_mask_to_label(mask_rgb: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    FloodNet dùng mask dạng grayscale:\n",
        "    - 0,1,3,5,6,9,... chính là class_id\n",
        "    Mask thường load ra (H,W,3) nhưng cả 3 kênh đều = nhau (x,x,x),\n",
        "    nên chỉ cần lấy 1 kênh là đủ.\n",
        "    \"\"\"\n",
        "    if mask_rgb.ndim == 3:\n",
        "        gray = mask_rgb[:, :, 0]\n",
        "    else:\n",
        "        gray = mask_rgb  # đã là (H,W)\n",
        "\n",
        "    return gray.astype(np.int64)"
      ],
      "metadata": {
        "id": "zPWpYqnG2Zzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiền xử lý dataset"
      ],
      "metadata": {
        "id": "9_G47U6X3Pc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FloodNetDataset(Dataset):\n",
        "    def __init__(self, root_dir, split=\"train\", processor=None, val_split=0.2):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split.lower()\n",
        "        self.processor = processor\n",
        "        self.has_mask = True  # Luôn có mask vì dùng từ labeled\n",
        "\n",
        "        # Lấy tất cả ảnh labeled từ train\n",
        "        img_dir_f = os.path.join(root_dir, \"train\", \"labeled\", \"flooded\", \"img\")\n",
        "        img_dir_nf = os.path.join(root_dir, \"train\", \"labeled\", \"non-flooded\", \"img\")\n",
        "        all_images = []\n",
        "        for d in [img_dir_f, img_dir_nf]:\n",
        "            if os.path.exists(d):\n",
        "                all_images += [f for f in os.listdir(d) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        # Chia ngẫu nhiên: train/val (seed để reproducible)\n",
        "        import random\n",
        "        random.seed(42)  # Fix seed để kết quả ổn định\n",
        "        random.shuffle(all_images)\n",
        "        split_idx = int(len(all_images) * (1 - val_split))\n",
        "\n",
        "        if self.split == \"train\":\n",
        "            self.image_files = all_images[:split_idx]  # 80%\n",
        "        else:  # validation\n",
        "            self.image_files = all_images[split_idx:]  # 20%\n",
        "\n",
        "        print(f\"[{split.upper()}] Found {len(self.image_files)} images (from labeled split)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "\n",
        "        # === TRAIN ===\n",
        "        if self.split == \"train\":\n",
        "            # flooded img?\n",
        "            flooded_path = os.path.join(self.root_dir, \"train/labeled/flooded/img\", img_name)\n",
        "            if os.path.exists(flooded_path):\n",
        "                img_path = flooded_path\n",
        "                mask_dir = os.path.join(self.root_dir, \"train/labeled/flooded/mask\")\n",
        "            else:\n",
        "                # non-flooded img?\n",
        "                img_path = os.path.join(self.root_dir, \"train/labeled/non-flooded/img\", img_name)\n",
        "                mask_dir = os.path.join(self.root_dir, \"train/labeled/non-flooded/mask\")\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                raise FileNotFoundError(f\"Train image not found: {img_path}\")\n",
        "\n",
        "            # mask thật\n",
        "            mask_path = os.path.join(mask_dir, f\"{os.path.splitext(img_name)[0]}_lab.png\")\n",
        "            if not os.path.exists(mask_path):\n",
        "                raise FileNotFoundError(f\"Train mask not found: {mask_path}\")\n",
        "\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            mask_rgb = np.array(Image.open(mask_path).convert(\"RGB\"))\n",
        "            mask_label = rgb_mask_to_label(mask_rgb)\n",
        "\n",
        "            # Encode train\n",
        "            encoded = self.processor(\n",
        "                images=image,\n",
        "                segmentation_maps=mask_label,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            return {k: v.squeeze(0) for k, v in encoded.items()}\n",
        "\n",
        "        # === VALIDATION ===\n",
        "        else:\n",
        "            img_path = os.path.join(self.root_dir, \"validation/img\", img_name)\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                raise FileNotFoundError(f\"Validation image not found: {img_path}\")\n",
        "\n",
        "            # pseudo mask\n",
        "            pseudo_path = os.path.join(\n",
        "                self.root_dir,\n",
        "                \"validation/pseudo_mask\",\n",
        "                f\"{os.path.splitext(img_name)[0]}_lab.png\"\n",
        "            )\n",
        "\n",
        "            if not os.path.exists(pseudo_path):\n",
        "                # Nếu không có pseudo mask -> bỏ qua ảnh này\n",
        "                return None\n",
        "\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            mask_rgb = np.array(Image.open(pseudo_path).convert(\"RGB\"))\n",
        "            mask_label = rgb_mask_to_label(mask_rgb)\n",
        "\n",
        "            encoded = self.processor(\n",
        "                images=image,\n",
        "                segmentation_maps=mask_label,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            return {k: v.squeeze(0) for k, v in encoded.items()}"
      ],
      "metadata": {
        "id": "0VFtoI6C3P2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "LjLZRzjo3oku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    root_dir = \"/content/drive/MyDrive/CODE/KHKT/FloodNet/\"\n",
        "\n",
        "    processor = SegformerImageProcessor.from_pretrained(\n",
        "        \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "        do_reduce_labels=False,\n",
        "        size={\"height\": 512, \"width\": 512}\n",
        "    )\n",
        "\n",
        "    # === TRAIN DATASET (bắt buộc) ===\n",
        "    train_dataset = FloodNetDataset(root_dir, split=\"train\", processor=processor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True,\n",
        "                              num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "    # === VALIDATION DATASET (tùy chọn - chỉ tạo nếu có ảnh) ===\n",
        "    val_images = glob.glob(f\"{root_dir}/validation/img/*.jpg\") + glob.glob(f\"{root_dir}/validation/img/*.png\")\n",
        "    if len(val_images) == 0:\n",
        "        print(\"Không tìm thấy ảnh validation -> chỉ train, không validate\")\n",
        "        val_loader = None\n",
        "        val_size = 0\n",
        "    else:\n",
        "        print(f\"Tìm thấy {len(val_images)} ảnh validation -> tạo val_dataset\")\n",
        "        val_dataset = FloodNetDataset(root_dir, split=\"validation\", processor=processor)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False,\n",
        "                                num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "        val_size = len(val_dataset)\n",
        "\n",
        "    print(f\"Train: {len(train_dataset)} ảnh | Validation: {val_size} ảnh\")\n",
        "\n",
        "    # === MODEL ===\n",
        "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "        num_labels = 10,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    ).cuda()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=8e-5, weight_decay=0.01)\n",
        "    total_steps = len(train_loader) * 15\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-4, total_steps=total_steps, pct_start=0.1)\n",
        "\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "    # === TRAINING LOOP ===\n",
        "    for epoch in range(1, 16):  # 15 epochs\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/15 [TRAIN]\"):\n",
        "            outputs = model(pixel_values=batch[\"pixel_values\"].cuda(),\n",
        "                          labels=batch[\"labels\"].cuda())\n",
        "            loss = outputs.loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"EPOCH {epoch}/15 | Train Loss: {avg_train_loss:.4f}\\n\")\n",
        "\n",
        "        # === LƯU MODEL ===\n",
        "        if epoch % 3 == 0 or epoch == 15:\n",
        "            save_path = f\"checkpoints/segformer_floodnet_epoch{epoch}\"\n",
        "            model.save_pretrained(save_path)\n",
        "            processor.save_pretrained(save_path)\n",
        "            print(f\"Đã lưu: {save_path}\")\n",
        "\n",
        "        # Luôn lưu latest\n",
        "        model.save_pretrained(\"checkpoints/latest_segformer\")\n",
        "        processor.save_pretrained(\"checkpoints/latest_segformer\")\n",
        "\n",
        "    print(\"\\nHOÀN TẤT! Model tốt nhất thường là epoch 12–15\")\n",
        "    print(\"→ Dùng: checkpoints/segformer_floodnet_epoch12 hoặc epoch15 để inference\")"
      ],
      "metadata": {
        "id": "IzJ5fahy3p_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "VOFFG2qClnVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Tạo folder lưu checkpoint\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "    train()"
      ],
      "metadata": {
        "id": "P_3KXc6K31Bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4919b58-a1a4-480a-9f46-073171a47200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN] Found 318 images (from labeled split)\n",
            "Tìm thấy 454 ảnh validation → tạo val_dataset\n",
            "[VALIDATION] Found 80 images (from labeled split)\n",
            "Train: 318 ảnh | Validation: 80 ảnh\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([10, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 1/15 | Train Loss: 2.0525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 2/15 | Train Loss: 1.2351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 3/15 | Train Loss: 0.7841\n",
            "Đã lưu: checkpoints/segformer_floodnet_epoch3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 4/15 | Train Loss: 0.6155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 5/15 | Train Loss: 0.5483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 6/15 | Train Loss: 0.4839\n",
            "Đã lưu: checkpoints/segformer_floodnet_epoch6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 7/15 | Train Loss: 0.4502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 8/15 | Train Loss: 0.4014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 9/15 | Train Loss: 0.3671\n",
            "Đã lưu: checkpoints/segformer_floodnet_epoch9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 10/15 | Train Loss: 0.3384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 11/15 | Train Loss: 0.3182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 12/15 | Train Loss: 0.3045\n",
            "Đã lưu: checkpoints/segformer_floodnet_epoch12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 13/15 | Train Loss: 0.2979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/15 [TRAIN]: 100%|██████████| 40/40 [00:57<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 14/15 | Train Loss: 0.2952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/15 [TRAIN]: 100%|██████████| 40/40 [00:58<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 15/15 | Train Loss: 0.2998\n",
            "Đã lưu: checkpoints/segformer_floodnet_epoch15\n",
            "\n",
            "HOÀN TẤT! Model tốt nhất thường là epoch 12–15\n",
            "→ Dùng: checkpoints/segformer_floodnet_epoch12 hoặc epoch15 để inference\n"
          ]
        }
      ]
    }
  ]
}